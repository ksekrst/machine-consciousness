## Why This Matters

Large language models (LLMs) hallucinate - generating outputs that are not grounded in data. As Dziri et al. (2023) define it,[^1] a hallucination is a response that can’t be fully verified, even if it sounds plausible.

Some hallucinations go further: they include **emotions**, **opinions**, or **claims of subjective experience** - statements that resemble *qualia* (the raw “what-it’s-like” subjective experience of consciousness).

This paper[^2] argues that:

> **A sufficiently sophisticated hallucination could be indistinguishable from an emergent mind.**

As AI systems become more fluent and expressive, the line between simulation and sentience may vanish.


## How to Cite

```bibtex
@article{sekrst2025hallucinations,
  title={Do Large Language Models Hallucinate Electric Fata Morganas?},
  author={Šekrst, Kristina},
  journal={Journal of Consciousness Studies},
  year={2025},
  note={Forthcoming}
}
```
## References

[^1]: Dziri, N., Milton, S., Yu, M., Zaiane, O. & Reddy, S. (2023). "On the Origin of Hallucinations in Conversational Models: Is It the Datasets or the Models?" In *Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, Association for Computational Linguistics, pp. 5271–5285. [https://aclanthology.org/2022.naacl-main.387.pdf](https://aclanthology.org/2022.naacl-main.387.pdf)
[^2]: Šekrst, K. (2025): "Do Large Language Models Dream of Electric Fata Morganas?" *Journal of Consciousness Studies*, forthcoming.
